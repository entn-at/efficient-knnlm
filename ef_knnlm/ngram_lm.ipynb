{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['a', 'b', 'c', 'd', 'e', 'b', 'c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b', 'c'),\n",
       " ('b', 'c', 'd'),\n",
       " ('c', 'd', 'e'),\n",
       " ('d', 'e', 'b'),\n",
       " ('e', 'b', 'c')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(text, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',),\n",
       " ('b',),\n",
       " ('c',),\n",
       " ('d',),\n",
       " ('e',),\n",
       " ('b',),\n",
       " ('c',),\n",
       " ('a', 'b'),\n",
       " ('b', 'c'),\n",
       " ('c', 'd'),\n",
       " ('d', 'e'),\n",
       " ('e', 'b'),\n",
       " ('b', 'c'),\n",
       " ('a', 'b', 'c'),\n",
       " ('b', 'c', 'd'),\n",
       " ('c', 'd', 'e'),\n",
       " ('d', 'e', 'b'),\n",
       " ('e', 'b', 'c')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(text, max_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lm_data(file):\n",
    "    res = ['</s>']\n",
    "    with open(file) as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            if i % 100000 == 0:\n",
    "                print(f'procesed {i} lines')\n",
    "            for tok in line.strip().split():\n",
    "                res.append(tok)\n",
    "                \n",
    "            res.append('</s>')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = '/projects/junxianh/knnlmXS/examples/language_model/wikitext-103/wiki.train.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesed 0 lines\n",
      "procesed 100000 lines\n",
      "procesed 200000 lines\n",
      "procesed 300000 lines\n",
      "procesed 400000 lines\n",
      "procesed 500000 lines\n",
      "procesed 600000 lines\n",
      "procesed 700000 lines\n",
      "procesed 800000 lines\n",
      "procesed 900000 lines\n",
      "procesed 1000000 lines\n",
      "procesed 1100000 lines\n",
      "procesed 1200000 lines\n",
      "procesed 1300000 lines\n",
      "procesed 1400000 lines\n",
      "procesed 1500000 lines\n",
      "procesed 1600000 lines\n",
      "procesed 1700000 lines\n",
      "procesed 1800000 lines\n"
     ]
    }
   ],
   "source": [
    "train_d = read_lm_data(train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103227022"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1677048"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(\"jxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 3\n",
    "train = everygrams(train_d, max_len=ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KneserNeyInterpolated(order=ngram, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267736"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267736"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Chronicles',)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(train.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit([train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypos = []\n",
    "with open('../analysis.jsonl') as fin:\n",
    "    for line in fin:\n",
    "        hypos.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.736425747686383\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "scores = 0\n",
    "cnt = 0\n",
    "lambda_ = 0.75\n",
    "ndict = 267744\n",
    "for hypo in hypos:\n",
    "    knn_scores = np.array(hypo['knn_scores'])\n",
    "    lm_scores = np.array(hypo['lm_scores'])\n",
    "    combine = logsumexp(np.stack((knn_scores + np.log(1-lambda_), lm_scores+np.log(lambda_)), axis=-1), axis=-1)\n",
    "    scores += combine.sum()\n",
    "    cnt += len(hypo['knn_scores'])\n",
    "    \n",
    "print(np.exp(-scores / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hypos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 hypos\n",
      "processed 10 hypos\n",
      "processed 20 hypos\n",
      "processed 30 hypos\n",
      "processed 40 hypos\n",
      "processed 50 hypos\n",
      "processed 60 hypos\n",
      "processed 70 hypos\n",
      "processed 80 hypos\n",
      "processed 90 hypos\n",
      "processed 100 hypos\n",
      "processed 110 hypos\n",
      "processed 120 hypos\n",
      "processed 130 hypos\n",
      "processed 140 hypos\n",
      "processed 150 hypos\n",
      "processed 160 hypos\n",
      "processed 170 hypos\n",
      "processed 180 hypos\n",
      "processed 190 hypos\n",
      "processed 200 hypos\n",
      "processed 210 hypos\n",
      "processed 220 hypos\n",
      "processed 230 hypos\n",
      "processed 240 hypos\n",
      "processed 250 hypos\n",
      "processed 260 hypos\n",
      "processed 270 hypos\n",
      "processed 280 hypos\n",
      "processed 290 hypos\n",
      "processed 300 hypos\n",
      "processed 310 hypos\n",
      "processed 320 hypos\n",
      "processed 330 hypos\n",
      "processed 340 hypos\n",
      "processed 350 hypos\n",
      "processed 360 hypos\n",
      "processed 370 hypos\n",
      "processed 380 hypos\n",
      "processed 390 hypos\n",
      "processed 400 hypos\n",
      "processed 410 hypos\n",
      "processed 420 hypos\n",
      "processed 430 hypos\n",
      "processed 440 hypos\n",
      "processed 450 hypos\n",
      "processed 460 hypos\n",
      "processed 470 hypos\n",
      "processed 480 hypos\n",
      "processed 490 hypos\n",
      "processed 500 hypos\n"
     ]
    }
   ],
   "source": [
    "valid_d = []\n",
    "ngram_scores = []\n",
    "prev = []\n",
    "\n",
    "for i, hypo in enumerate(hypos):\n",
    "    local_scores = []\n",
    "    if i % 10 == 0:\n",
    "        print(f'processed {i} hypos')\n",
    "    for tok in hypo['string']:\n",
    "        prev = prev[-(ngram-1):]\n",
    "        local_scores.append(lm.logscore(tok, prev) * np.log(2))\n",
    "        prev.append(tok)\n",
    "        \n",
    "    ngram_scores.append(local_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.281385193690674, -1.726526913352188, -0.10818972796825614, -4.116738447381035, -1.8575129807038586, -6.301257375983728, -19.155008273892328, -16.055597408833552, -15.088023864807079, -6.502567791444007, -2.1866111495064295, -1.664907684801027, -11.88072916275888, -3.5768769471175963, -8.446035348312261, -5.171614862915676, -7.578745271452401, -0.0030389433221611065, -1.6749315955574957, -10.417013476840813]\n"
     ]
    }
   ],
   "source": [
    "print(ngram_scores[2][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.78125, -0.76025390625, -0.02447509765625, -0.004161834716796875, -0.189208984375, -3.69921875, -1.193359375, -0.0078887939453125, -1.0322265625, -4.99609375, -0.031524658203125, -0.493408203125, -11.1171875, -2.58984375, -3.1640625, -1.048828125, -2.126953125, -0.00103759765625, -2.3125, -5.53125]\n"
     ]
    }
   ],
   "source": [
    "print(hypos[2]['lm_scores'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.30859375, -1.69140625, -0.108154296875, -0.0980224609375, -0.17529296875, -3.09375, -3.49609375, -10000.0, -0.890625, -2.916015625, -0.5634765625, -0.98876953125, -10000.0, -2.72265625, -5.40234375, -2.396484375, -2.654296875, -0.0022125244140625, -1.00390625, -4.59375]\n"
     ]
    }
   ],
   "source": [
    "print(hypos[2]['knn_scores'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.193501617872233\n"
     ]
    }
   ],
   "source": [
    "scores = 0\n",
    "cnt = 0\n",
    "lambda_ = 0.9\n",
    "ndict = 267744\n",
    "for hypo, ngram_s in zip(hypos, ngram_scores):\n",
    "    ngram_lm_scores = np.array(ngram_s)\n",
    "    lm_scores = np.array(hypo['lm_scores'])\n",
    "    combine = logsumexp(np.stack((ngram_lm_scores + np.log(1-lambda_), lm_scores+np.log(lambda_)), axis=-1), axis=-1)\n",
    "    scores += combine.sum()\n",
    "    cnt += len(hypo['lm_scores'])\n",
    "    \n",
    "print(np.exp(-scores / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ngrams(valid_d, ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.perplexity(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = 0\n",
    "cnt = 0\n",
    "lambda_ = 0.75\n",
    "ndict = 267744\n",
    "for hypo in hypos:\n",
    "    knn_scores = np.array(hypo['knn_scores'])\n",
    "    lm_scores = np.array(hypo['lm_scores'])\n",
    "    combine = logsumexp(np.stack((knn_scores + np.log(1-lambda_), lm_scores+np.log(lambda_)), axis=-1), axis=-1)\n",
    "    scores += combine.sum()\n",
    "    cnt += len(hypo['knn_scores'])\n",
    "    \n",
    "print(np.exp(-scores / cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
